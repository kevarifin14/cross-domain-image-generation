{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8224405696631393575\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 13500416\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 17807982321921145056\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn as skl\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import scipy.io as scio\n",
    "import scipy\n",
    "import skimage as sk\n",
    "import skimage.io as skio\n",
    "from skimage import color\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import h5py\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, Conv2DTranspose\n",
    "from keras.layers.core import Flatten, Reshape, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input\n",
    "from PIL import Image\n",
    "import math\n",
    "from tensorflow.python.client import device_lib\n",
    "%matplotlib inline\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(mnist_X_train, mnist_y_train), (mnist_X_test, mnist_y_test) = mnist.load_data()\n",
    "svhn_train = scio.loadmat('svhn_train.mat')\n",
    "svhn_test = scio.loadmat('svhn_test.mat')\n",
    "svhn_X_train = svhn_train['X']\n",
    "svhn_y_train = svhn_train['y']\n",
    "svhn_X_test = svhn_test['X']\n",
    "svhn_y_test = svhn_test['y']\n",
    "svhn_X_train = np.array([svhn_X_train[:, :, :, i] for i in range(svhn_X_train.shape[-1])])\n",
    "svhn_X_test = np.array([svhn_X_test[:, :, :, i] for i in range(svhn_X_test.shape[-1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def content_extracdtor():\n",
    "    \n",
    "\n",
    "def generator():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3, 3), input_shape=(32, 32, 3), padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3)))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Conv2D(64, (5, 5), padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Conv2D(1, (5, 5), padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    return model\n",
    "\n",
    "def discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', input_shape=(28, 28, 1), kernel_initializer='glorot_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', kernel_initializer='glorot_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(1, (4, 4), padding='same', kernel_initializer='glorot_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    return model\n",
    "\n",
    "def generator_with_discriminator(g, d):\n",
    "    model = Sequential()\n",
    "    model.add(g)\n",
    "    d.trainable = False\n",
    "    model.add(d)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(mnist_X_train, svhn_X_train, g=None, d=None, batch_size=128, epochs=100):\n",
    "    mnist_X_train = (mnist_X_train.astype(np.float32) - 127.5)/127.5\n",
    "    svhn_X_train = (svhn_X_train.astype(np.float32) - 127.5)/127.5\n",
    "    if not g: g = generator()\n",
    "    if not d: d = discriminator()\n",
    "    g_d = generator_with_discriminator(g, d)\n",
    "    g_d_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "    g_d.compile(loss='binary_crossentropy', optimizer=g_d_optim)\n",
    "    g.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "    d.trainable = True\n",
    "    d_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "    d.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
    "    for e in range(epochs):\n",
    "        print('Epoch: {}'.format(e))\n",
    "        for i in range(int(mnist_X_train.shape[0] / batch_size)):\n",
    "            # Create batches\n",
    "#             batch_rand_X = np.random.uniform(-1, 1, size=(batch_size, 32, 32, 3))\n",
    "            batch_mnist_X = mnist_X_train[np.random.randint(mnist_X_train.shape[0], size=batch_size)]\n",
    "            batch_svhn_X = svhn_X_train[np.random.randint(svhn_X_train.shape[0], size=batch_size)]\n",
    "            \n",
    "        \n",
    "            # Generate MNIST examples from noise for training\n",
    "            g_train_data = g.predict(batch_svhn_X, verbose=0)\n",
    "            \n",
    "            # Label as real or fake\n",
    "            y_real, y_fake = np.ones(batch_size), np.zeros(batch_size)\n",
    "            X_train, y_train = np.vstack((batch_mnist_X, g_train_data)), np.hstack((y_real, y_fake))\n",
    "            # Train discriminator on this real/fake data\n",
    "            d_loss = d.train_on_batch(X_train, y_train)\n",
    "            \n",
    "            # Train generator based on the outputs of the discriminator\n",
    "#             batch_rand_X = np.random.uniform(-1, 1, size=(batch_size, 32, 32, 3))\n",
    "            batch_svhn_X = svhn_X_train[np.random.randint(svhn_X_train.shape[0], size=batch_size)]\n",
    "            \n",
    "            d.trainable = False\n",
    "            g_loss = g_d.train_on_batch(batch_svhn_X, np.ones(batch_svhn_X.shape[0]))\n",
    "            d.trainable = True\n",
    "            if i % 100 == 99:\n",
    "                print('Iteration {}'.format(i))\n",
    "                print('D Loss:', d_loss)\n",
    "                print('G Loss:', g_loss)\n",
    "                g.save_weights('G.h5')\n",
    "                d.save_weights('D.h5')\n",
    "    return g, d\n",
    "\n",
    "def test_pred(g, d):\n",
    "    \n",
    "    g_images = g.predict(svhn_X_train[np.random.randint(svhn_X_train.shape[0], size=10)])\n",
    "\n",
    "#     g_images = g.predict(np.random.uniform(-1, 1, size=(10, 32, 32, 3)))\n",
    "    g_images = g_images * 127.5 + 127.5\n",
    "    for i in range(10):\n",
    "        plt.figure()\n",
    "        plt.figure()\n",
    "        plt.imshow(g_images[i][:, :, 0], cmap='gray')\n",
    "    print(d.predict(g_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-5-860aa1453c88>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-860aa1453c88>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    g, d = train(mnist_X_train[:, :, :, None], svhn_X_train, epochs=10\u001b[0m\n\u001b[0m                                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "g, d = train(mnist_X_train[:, :, :, None], svhn_X_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ";"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
